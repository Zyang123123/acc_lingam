{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Evw1qoakxycq",
    "outputId": "ec2c0424-5bfb-447a-e8a9-47a5874bbda7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.26.4', '2.2.2', '0.20.3', '2.0.6']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import graphviz\n",
    "import lingam\n",
    "from lingam.utils import make_dot, print_causal_directions, print_dagc\n",
    "\n",
    "print([np.__version__, pd.__version__, graphviz.__version__, lingam.__version__])\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7plP3RhZ0vhN",
    "outputId": "3ea4b0a1-11c5-4869-c608-12eb65ed4b49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0        1         2        3         4         5         6  \\\n",
      "0    -2.69100 -1.95910 -2.552400 -1.25590 -1.339600  2.101700 -1.059000   \n",
      "1    -1.69740  3.19930  0.043200 -0.94342 -4.372200 -0.646490 -0.855830   \n",
      "2     0.90524  4.19570  1.362800 -0.89663 -1.273800 -1.525000  0.201560   \n",
      "3     3.12950  3.01960  0.008149 -1.12460 -0.125310 -1.298800 -0.684750   \n",
      "4     2.22200 -2.78580 -0.721280 -0.59846  4.871900 -0.999250 -0.282030   \n",
      "...       ...      ...       ...      ...       ...       ...       ...   \n",
      "1195 -0.43135 -4.33950 -2.437800  2.70770  3.537600 -0.174670  1.650500   \n",
      "1196 -6.28480 -0.79428 -3.976800 -0.47824 -3.462500 -0.670610  0.496270   \n",
      "1197 -3.68720 -0.48170  0.110430 -1.67120 -4.018600  0.076852 -0.032055   \n",
      "1198 -0.84579 -2.76220 -1.165800 -1.14920 -0.721700 -1.647100 -0.113880   \n",
      "1199  2.19910 -4.00140  2.514400 -2.04100  0.031647 -1.483500  1.455500   \n",
      "\n",
      "            7        8       9  \n",
      "0    -0.98590 -2.14800 -1.7483  \n",
      "1    -1.97830 -0.84755  1.5025  \n",
      "2     0.20549 -0.33146  2.6398  \n",
      "3     0.48168 -0.57447 -1.6925  \n",
      "4    -1.22090 -0.35563 -3.0108  \n",
      "...       ...      ...     ...  \n",
      "1195 -1.27500  0.20823 -0.8712  \n",
      "1196 -1.28200  2.44020  1.0585  \n",
      "1197  0.53831  4.31330  4.5867  \n",
      "1198  4.91040  2.67740  4.2893  \n",
      "1199 -0.74004 -1.63980  1.2634  \n",
      "\n",
      "[1200 rows x 10 columns]\n",
      "Execution time: 2.4597830772399902 seconds\n",
      "[5, 6, 2, 0, 7, 8, 9, 3, 4, 1]\n",
      "[[ 0.     0.     0.118  0.     0.    -0.096  0.     0.     0.     0.   ]\n",
      " [ 0.224  0.     0.191  0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.256  0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.158  0.     0.     0.241  0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.224  0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.233  0.     0.     0.105  0.272  0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.297  0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.223  0.     0.     0.254  0.   ]]\n",
      "[[ 0.459  0.     0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.455  0.    -0.068  0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.085  0.     0.527  0.071  0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.526  0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.395  0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.434  0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.071  0.394  0.093  0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.    -0.095  0.275  0.081  0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.438  0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.415]]\n",
      "[[1 1 0 0 1 0 0 0 0 0]\n",
      " [0 1 1 0 0 0 0 0 0 0]\n",
      " [0 0 1 1 0 0 0 1 0 0]\n",
      " [0 0 0 1 1 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 1 0 0 1]\n",
      " [0 0 0 0 0 0 1 1 0 0]\n",
      " [0 0 0 0 0 0 0 1 1 0]\n",
      " [0 0 0 0 0 0 0 0 1 1]\n",
      " [0 0 0 0 0 0 0 0 0 1]]\n",
      "(10, 10)\n",
      "B0 result:\n",
      "[[ 0.     0.     0.118  0.     0.    -0.096  0.     0.     0.     0.   ]\n",
      " [ 0.224  0.     0.191  0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.256  0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.158  0.     0.     0.241  0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.224  0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.233  0.     0.     0.105  0.272  0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.297  0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.223  0.     0.     0.254  0.   ]]\n",
      "B1 result:\n",
      "[[ 0.459  0.     0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.455  0.    -0.068  0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.085  0.     0.527  0.071  0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.526  0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.395  0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.434  0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.071  0.394  0.093  0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.    -0.095  0.275  0.081  0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.438  0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.415]]\n",
      "B2 result:\n",
      "[[-0.357  0.     0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.    -0.252  0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.    -0.053 -0.169  0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.    -0.189  0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.    -0.135  0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.    -0.245  0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.    -0.065  0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.    -0.262  0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.    -0.23   0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.    -0.153]]\n",
      "B result:\n",
      "[[0.816 0.    0.118 0.    0.    0.096 0.    0.    0.    0.   ]\n",
      " [0.224 0.708 0.191 0.068 0.    0.    0.    0.    0.    0.   ]\n",
      " [0.085 0.053 0.696 0.071 0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.256 0.715 0.    0.    0.    0.    0.    0.   ]\n",
      " [0.158 0.    0.    0.241 0.53  0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.68  0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.295 0.459 0.093 0.    0.   ]\n",
      " [0.    0.    0.233 0.    0.    0.105 0.367 0.537 0.081 0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.297 0.668 0.   ]\n",
      " [0.    0.    0.    0.    0.    0.223 0.    0.    0.254 0.568]]\n",
      "B result:\n",
      "[[1 0 1 0 0 0 0 0 0 0]\n",
      " [1 1 1 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 1 1 0 0 0 0 0 0]\n",
      " [1 0 0 1 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 1 1 0 0 0]\n",
      " [0 0 1 0 0 1 1 1 0 0]\n",
      " [0 0 0 0 0 0 0 1 1 0]\n",
      " [0 0 0 0 0 1 0 0 1 1]]\n",
      "TP:11\n",
      "FP:12\n",
      "FN:10\n",
      "TN:67\n",
      "Precision B0: 0.4782608695652174\n",
      "Recall B0: 0.5238095238095238\n",
      "F1-score B0: 0.5\n",
      "Accuracy B0: 0.78\n",
      "Structual Hamming Distance of B0: 22\n",
      "Error rate B0: 0.22\n",
      "Accuracy B0: 0.78\n",
      "B0 result:\n",
      "[[ 0.     0.     0.118  0.     0.    -0.096  0.     0.     0.     0.   ]\n",
      " [ 0.224  0.     0.191  0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.256  0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.158  0.     0.     0.241  0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.224  0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.233  0.     0.     0.105  0.272  0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.297  0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.223  0.     0.     0.254  0.   ]]\n",
      "B1 result:\n",
      "[[ 0.459  0.     0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.455  0.    -0.068  0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.085  0.     0.527  0.071  0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.526  0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.395  0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.434  0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.071  0.394  0.093  0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.    -0.095  0.275  0.081  0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.438  0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.415]]\n",
      "B2 result:\n",
      "[[-0.357  0.     0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.    -0.252  0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.    -0.053 -0.169  0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.    -0.189  0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.    -0.135  0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.    -0.245  0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.    -0.065  0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.    -0.262  0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.    -0.23   0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.    -0.153]]\n",
      "B result:\n",
      "[[0.459 0.    0.118 0.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.224 0.455 0.191 0.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.085 0.    0.527 0.071 0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.256 0.526 0.    0.    0.    0.    0.    0.   ]\n",
      " [0.158 0.    0.    0.241 0.395 0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.434 0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.224 0.394 0.093 0.    0.   ]\n",
      " [0.    0.    0.233 0.    0.    0.105 0.272 0.275 0.081 0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.297 0.438 0.   ]\n",
      " [0.    0.    0.    0.    0.    0.223 0.    0.    0.254 0.415]]\n",
      "B result:\n",
      "[[1 0 1 0 0 0 0 0 0 0]\n",
      " [1 1 1 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 1 1 0 0 0 0 0 0]\n",
      " [1 0 0 1 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 1 1 0 0 0]\n",
      " [0 0 1 0 0 1 1 1 0 0]\n",
      " [0 0 0 0 0 0 0 1 1 0]\n",
      " [0 0 0 0 0 1 0 0 1 1]]\n",
      "TP:11\n",
      "FP:12\n",
      "FN:10\n",
      "TN:67\n",
      "Precision B0: 0.4782608695652174\n",
      "Recall B0: 0.5238095238095238\n",
      "F1-score B0: 0.5\n",
      "Accuracy B0: 0.78\n",
      "Structual Hamming Distance of B0: 22\n",
      "Error rate B0: 0.22\n",
      "Accuracy B0: 0.78\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = pd.read_csv('./examples/data/data_benchmark_1/fMRI_processed_by_Nauta/returns/timeseries6.csv')\n",
    "print(X)\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "model = lingam.VARLiNGAM(lags=2)\n",
    "model.fit(X)\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(\"Execution time:\", execution_time, \"seconds\")\n",
    "\n",
    "\n",
    "print(model.causal_order_)\n",
    "\n",
    "\n",
    "# B0\n",
    "B0_result=model.adjacency_matrices_[0]\n",
    "print(B0_result)\n",
    "\n",
    "\n",
    "# B1\n",
    "B1_result=model.adjacency_matrices_[1]\n",
    "print(B1_result)\n",
    "\n",
    "# # B2\n",
    "# B2_result=model.adjacency_matrices_[2]\n",
    "# print(B2_result)\n",
    "\n",
    "\n",
    "\n",
    "def precision_recall_f1(matrix1, matrix2):\n",
    "    assert matrix1.shape == matrix2.shape, \"Matrices must have the same shape\"\n",
    "\n",
    "    # Convert matrices to boolean: True for non-zero, False for zero\n",
    "    bool_matrix1 = matrix1 != 0\n",
    "    bool_matrix2 = matrix2 != 0\n",
    "\n",
    "    # Calculate True Positives (TP), False Positives (FP), False Negatives (FN), True Negatives (TN)\n",
    "    TP = np.sum(np.logical_and(bool_matrix1, bool_matrix2))\n",
    "    print(f\"TP:{TP}\")\n",
    "    FP = np.sum(np.logical_and(np.logical_not(bool_matrix1), bool_matrix2))\n",
    "    print(f\"FP:{FP}\")\n",
    "    FN = np.sum(np.logical_and(bool_matrix1, np.logical_not(bool_matrix2)))\n",
    "    print(f\"FN:{FN}\")\n",
    "    TN = np.sum(np.logical_and(np.logical_not(bool_matrix1), np.logical_not(bool_matrix2)))\n",
    "    print(f\"TN:{TN}\")\n",
    "\n",
    "    # Compute precision, recall, and f1-score\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
    "\n",
    "    return precision, recall, f1_score,accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the CSV file\n",
    "#file_path = 'examples/data/data_benchmark_1/FinanceCPT/ground_truth/random-rels_20_1C.csv'  # Change this to your actual file path\n",
    "file_path = 'examples/data/data_benchmark_1/fMRI_processed_by_Nauta/ground_truth/sim6_gt_processed.csv'  # Change this to your actual file path\n",
    "data = pd.read_csv(file_path, header=None)\n",
    "data.columns = ['cause', 'effect', 'edge']\n",
    "\n",
    "# Determine the size of the causality matrix\n",
    "num_causes = data['cause'].max() + 1\n",
    "num_effects = data['effect'].max() + 1\n",
    "dim=max(num_causes, num_effects)\n",
    "# Create an empty matrix of appropriate size\n",
    "causality_matrix = np.zeros((dim, dim), dtype=int)\n",
    "\n",
    "# Populate the matrix with the 'edge' values\n",
    "for _, row in data.iterrows():\n",
    "    causality_matrix[row['effect'], row['cause']] = row['edge']\n",
    "\n",
    "# Display the causality matrix\n",
    "print(causality_matrix)\n",
    "print(causality_matrix.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "result = np.zeros_like(model.adjacency_matrices_[0])\n",
    "for idx, adj_matrix in enumerate(model.adjacency_matrices_):\n",
    "    #result=np.maximum(result, adj_matrix)\n",
    "    result += np.abs(adj_matrix)\n",
    "    print(f\"B{idx} result:\")\n",
    "    print(adj_matrix)\n",
    "#result = np.abs(B0_result) + np.abs(B1_result)# + np.abs(B2_result)\n",
    "print(\"B result:\")\n",
    "print(result)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Assume a threshold to binarize the result\n",
    "threshold = 0.1\n",
    "result = (result >= threshold).astype(int)\n",
    "print(\"B result:\")\n",
    "print(result)\n",
    "\n",
    "\n",
    "precision_B0, recall_B0, f1_score_B0, accuracy_B0 = precision_recall_f1(causality_matrix, result)\n",
    "print(f\"Precision B0: {precision_B0}\")\n",
    "print(f\"Recall B0: {recall_B0}\")\n",
    "print(f\"F1-score B0: {f1_score_B0}\")\n",
    "print(f\"Accuracy B0: {accuracy_B0}\")\n",
    "\n",
    "\n",
    "# Compute the structural hamming distance\n",
    "def structural_hamming_distance(matrix1, matrix2):\n",
    "    assert matrix1.shape == matrix2.shape, \"Matrices must have the same shape\"\n",
    "\n",
    "    # Convert matrices to boolean: True for non-zero, False for zero\n",
    "    bool_matrix1 = matrix1 != 0\n",
    "    bool_matrix2 = matrix2 != 0\n",
    "\n",
    "    # Compute the Hamming distance\n",
    "    distance = np.sum(bool_matrix1 != bool_matrix2)\n",
    "\n",
    "    return distance\n",
    "\n",
    "\n",
    "\n",
    "shd_B0 = structural_hamming_distance(causality_matrix, result)\n",
    "#print(causality_matrix)\n",
    "#print(B0_result)\n",
    "print(f\"Structual Hamming Distance of B0: {shd_B0}\")\n",
    "error_rate_B0=shd_B0/B0_result.shape[0]**2\n",
    "print(f\"Error rate B0: {error_rate_B0}\")\n",
    "print(f\"Accuracy B0: {1-error_rate_B0}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "result = np.zeros_like(model.adjacency_matrices_[0])\n",
    "for idx, adj_matrix in enumerate(model.adjacency_matrices_):\n",
    "    result=np.maximum(result, adj_matrix)\n",
    "    #result += np.abs(adj_matrix)\n",
    "    print(f\"B{idx} result:\")\n",
    "    print(adj_matrix)\n",
    "#result = np.abs(B0_result) + np.abs(B1_result)# + np.abs(B2_result)\n",
    "print(\"B result:\")\n",
    "print(result)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Assume a threshold to binarize the result\n",
    "threshold = 0.1\n",
    "result = (result >= threshold).astype(int)\n",
    "print(\"B result:\")\n",
    "print(result)\n",
    "\n",
    "\n",
    "precision_B0, recall_B0, f1_score_B0, accuracy_B0 = precision_recall_f1(causality_matrix, result)\n",
    "print(f\"Precision B0: {precision_B0}\")\n",
    "print(f\"Recall B0: {recall_B0}\")\n",
    "print(f\"F1-score B0: {f1_score_B0}\")\n",
    "print(f\"Accuracy B0: {accuracy_B0}\")\n",
    "\n",
    "\n",
    "# Compute the structural hamming distance\n",
    "def structural_hamming_distance(matrix1, matrix2):\n",
    "    assert matrix1.shape == matrix2.shape, \"Matrices must have the same shape\"\n",
    "\n",
    "    # Convert matrices to boolean: True for non-zero, False for zero\n",
    "    bool_matrix1 = matrix1 != 0\n",
    "    bool_matrix2 = matrix2 != 0\n",
    "\n",
    "    # Compute the Hamming distance\n",
    "    distance = np.sum(bool_matrix1 != bool_matrix2)\n",
    "\n",
    "    return distance\n",
    "\n",
    "\n",
    "\n",
    "shd_B0 = structural_hamming_distance(causality_matrix, result)\n",
    "#print(causality_matrix)\n",
    "#print(B0_result)\n",
    "print(f\"Structual Hamming Distance of B0: {shd_B0}\")\n",
    "error_rate_B0=shd_B0/B0_result.shape[0]**2\n",
    "print(f\"Error rate B0: {error_rate_B0}\")\n",
    "print(f\"Accuracy B0: {1-error_rate_B0}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X4aKsj9mOBcP",
    "outputId": "baf05d9f-9d4d-4001-8405-f0c24b69128c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimate VAR coefficients time: 0.0020949840545654297 seconds\n",
      "precomputation Execution time: 0.002003908157348633 seconds\n",
      "Measure method: pwling_v2\n",
      "search causal order Execution time: 0.0009992122650146484 seconds\n",
      "estimate adjacency matrix Execution time: 0.00699925422668457 seconds\n",
      "Estimate VAR coefficients time: 0.0020034313201904297 seconds\n",
      "precomputation Execution time: 0.0019996166229248047 seconds\n",
      "Measure method: pwling_v2\n",
      "search causal order Execution time: 0.0006539821624755859 seconds\n",
      "estimate adjacency matrix Execution time: 0.009284734725952148 seconds\n",
      "Estimate VAR coefficients time: 0.00099945068359375 seconds\n",
      "precomputation Execution time: 0.006000041961669922 seconds\n",
      "Measure method: pwling_v2\n",
      "search causal order Execution time: 0.0025103092193603516 seconds\n",
      "estimate adjacency matrix Execution time: 0.02456355094909668 seconds\n",
      "Estimate VAR coefficients time: 0.0025048255920410156 seconds\n",
      "precomputation Execution time: 0.00551915168762207 seconds\n",
      "Measure method: pwling_v2\n",
      "search causal order Execution time: 0.0 seconds\n",
      "estimate adjacency matrix Execution time: 0.019608497619628906 seconds\n",
      "Estimate VAR coefficients time: 0.001504659652709961 seconds\n",
      "precomputation Execution time: 0.0009999275207519531 seconds\n",
      "Measure method: pwling_v2\n",
      "search causal order Execution time: 0.00099945068359375 seconds\n",
      "estimate adjacency matrix Execution time: 0.00500035285949707 seconds\n",
      "Estimate VAR coefficients time: 0.001001119613647461 seconds\n",
      "precomputation Execution time: 0.0015091896057128906 seconds\n",
      "Measure method: pwling_v2\n",
      "search causal order Execution time: 0.0 seconds\n",
      "estimate adjacency matrix Execution time: 0.006999969482421875 seconds\n",
      "Estimate VAR coefficients time: 0.0020003318786621094 seconds\n",
      "precomputation Execution time: 0.0019626617431640625 seconds\n",
      "Measure method: pwling_v2\n",
      "search causal order Execution time: 0.0010485649108886719 seconds\n",
      "estimate adjacency matrix Execution time: 0.0071222782135009766 seconds\n",
      "Estimate VAR coefficients time: 0.0009999275207519531 seconds\n",
      "precomputation Execution time: 0.002506732940673828 seconds\n",
      "Measure method: pwling_v2\n",
      "search causal order Execution time: 0.00099945068359375 seconds\n",
      "estimate adjacency matrix Execution time: 0.00599980354309082 seconds\n",
      "Estimate VAR coefficients time: 0.002000093460083008 seconds\n",
      "precomputation Execution time: 0.006506204605102539 seconds\n",
      "Measure method: pwling_v2\n",
      "search causal order Execution time: 0.003000497817993164 seconds\n",
      "estimate adjacency matrix Execution time: 0.015648365020751953 seconds\n",
      "Estimate VAR coefficients time: 0.0009999275207519531 seconds\n",
      "precomputation Execution time: 0.002506256103515625 seconds\n",
      "Measure method: pwling_v2\n",
      "search causal order Execution time: 0.0 seconds\n",
      "estimate adjacency matrix Execution time: 0.00700068473815918 seconds\n",
      "Estimate VAR coefficients time: 0.005506753921508789 seconds\n",
      "precomputation Execution time: 0.003000497817993164 seconds\n",
      "Measure method: pwling_v2\n",
      "search causal order Execution time: 0.0009999275207519531 seconds\n",
      "estimate adjacency matrix Execution time: 0.008510828018188477 seconds\n",
      "Estimate VAR coefficients time: 0.001997709274291992 seconds\n",
      "precomputation Execution time: 0.0063631534576416016 seconds\n",
      "Measure method: pwling_v2\n",
      "search causal order Execution time: 0.0029997825622558594 seconds\n",
      "estimate adjacency matrix Execution time: 0.01927661895751953 seconds\n",
      "Estimate VAR coefficients time: 0.007508754730224609 seconds\n",
      "precomputation Execution time: 0.003000497817993164 seconds\n",
      "Measure method: pwling_v2\n",
      "search causal order Execution time: 0.0 seconds\n",
      "estimate adjacency matrix Execution time: 0.007445573806762695 seconds\n",
      "Estimate VAR coefficients time: 0.0 seconds\n",
      "precomputation Execution time: 0.004004001617431641 seconds\n",
      "Measure method: pwling_v2\n",
      "search causal order Execution time: 0.0 seconds\n",
      "estimate adjacency matrix Execution time: 0.005005836486816406 seconds\n",
      "Estimate VAR coefficients time: 0.001999378204345703 seconds\n",
      "precomputation Execution time: 0.0009999275207519531 seconds\n",
      "Measure method: pwling_v2\n",
      "search causal order Execution time: 0.0009996891021728516 seconds\n",
      "estimate adjacency matrix Execution time: 0.006507396697998047 seconds\n",
      "Estimate VAR coefficients time: 0.0 seconds\n",
      "precomputation Execution time: 0.004003763198852539 seconds\n",
      "Measure method: pwling_v2\n",
      "search causal order Execution time: 0.0 seconds\n",
      "estimate adjacency matrix Execution time: 0.00742340087890625 seconds\n",
      "Estimate VAR coefficients time: 0.0009996891021728516 seconds\n",
      "precomputation Execution time: 0.0009999275207519531 seconds\n",
      "Measure method: pwling_v2\n",
      "search causal order Execution time: 0.0009999275207519531 seconds\n",
      "estimate adjacency matrix Execution time: 0.004999637603759766 seconds\n",
      "Estimate VAR coefficients time: 0.0009999275207519531 seconds\n",
      "precomputation Execution time: 0.0021376609802246094 seconds\n",
      "Measure method: pwling_v2\n",
      "search causal order Execution time: 0.0009992122650146484 seconds\n",
      "estimate adjacency matrix Execution time: 0.005509138107299805 seconds\n",
      "Estimate VAR coefficients time: 0.0020008087158203125 seconds\n",
      "precomputation Execution time: 0.0009992122650146484 seconds\n",
      "Measure method: pwling_v2\n",
      "search causal order Execution time: 0.0 seconds\n",
      "estimate adjacency matrix Execution time: 0.006505489349365234 seconds\n",
      "Estimate VAR coefficients time: 0.0010027885437011719 seconds\n",
      "precomputation Execution time: 0.0020003318786621094 seconds\n",
      "Measure method: pwling_v2\n",
      "search causal order Execution time: 0.0009999275207519531 seconds\n",
      "estimate adjacency matrix Execution time: 0.0061664581298828125 seconds\n",
      "Estimate VAR coefficients time: 0.0009999275207519531 seconds\n",
      "precomputation Execution time: 0.0009996891021728516 seconds\n",
      "Measure method: pwling_v2\n",
      "search causal order Execution time: 0.0010008811950683594 seconds\n",
      "estimate adjacency matrix Execution time: 0.006506443023681641 seconds\n",
      "Estimate VAR coefficients time: 0.0020198822021484375 seconds\n",
      "precomputation Execution time: 0.01460576057434082 seconds\n",
      "Measure method: pwling_v2\n",
      "search causal order Execution time: 0.008506536483764648 seconds\n",
      "estimate adjacency matrix Execution time: 0.025174379348754883 seconds\n",
      "Estimate VAR coefficients time: 0.016457557678222656 seconds\n",
      "precomputation Execution time: 0.16007137298583984 seconds\n",
      "Measure method: pwling_v2\n",
      "search causal order Execution time: 0.19664430618286133 seconds\n",
      "estimate adjacency matrix Execution time: 0.17999958992004395 seconds\n",
      "Estimate VAR coefficients time: 0.0030012130737304688 seconds\n",
      "precomputation Execution time: 0.003507852554321289 seconds\n",
      "Measure method: pwling_v2\n",
      "search causal order Execution time: 0.0009968280792236328 seconds\n",
      "estimate adjacency matrix Execution time: 0.009511709213256836 seconds\n",
      "Estimate VAR coefficients time: 0.005502462387084961 seconds\n",
      "precomputation Execution time: 0.009507179260253906 seconds\n",
      "Measure method: pwling_v2\n",
      "search causal order Execution time: 0.0030031204223632812 seconds\n",
      "estimate adjacency matrix Execution time: 0.01957845687866211 seconds\n",
      "Estimate VAR coefficients time: 0.014045000076293945 seconds\n",
      "precomputation Execution time: 0.005560636520385742 seconds\n",
      "Measure method: pwling_v2\n",
      "search causal order Execution time: 0.0 seconds\n",
      "estimate adjacency matrix Execution time: 0.013528823852539062 seconds\n",
      "Estimate VAR coefficients time: 0.0025053024291992188 seconds\n",
      "precomputation Execution time: 0.0010039806365966797 seconds\n",
      "Measure method: pwling_v2\n",
      "search causal order Execution time: 0.001444101333618164 seconds\n",
      "estimate adjacency matrix Execution time: 0.0058345794677734375 seconds\n",
      "Estimate VAR coefficients time: 0.01151418685913086 seconds\n",
      "precomputation Execution time: 0.0039975643157958984 seconds\n",
      "Measure method: pwling_v2\n",
      "search causal order Execution time: 0.0009992122650146484 seconds\n",
      "estimate adjacency matrix Execution time: 0.014844179153442383 seconds\n",
      "    features  n_samples  SHD  error rate  accuracy  precision    recall  \\\n",
      "0          5        200    5    0.200000  0.800000   0.857143  0.600000   \n",
      "1          5        200    2    0.080000  0.920000   0.900000  0.900000   \n",
      "2         10        200   21    0.210000  0.790000   0.500000  0.523810   \n",
      "3         10        200   13    0.130000  0.870000   0.722222  0.619048   \n",
      "4          5        200    8    0.320000  0.680000   0.777778  0.538462   \n",
      "5          5        200    8    0.320000  0.680000   0.600000  0.600000   \n",
      "6          5        200   10    0.400000  0.600000   0.500000  0.500000   \n",
      "7          5        200    9    0.360000  0.640000   0.636364  0.583333   \n",
      "8         10        200   19    0.190000  0.810000   0.550000  0.523810   \n",
      "9          5        200    9    0.360000  0.640000   0.555556  0.500000   \n",
      "10         5       2400    5    0.200000  0.800000   0.727273  0.800000   \n",
      "11        10        200   17    0.170000  0.830000   0.611111  0.523810   \n",
      "12         5       2400    5    0.200000  0.800000   0.727273  0.800000   \n",
      "13         5        200    5    0.200000  0.800000   0.857143  0.600000   \n",
      "14         5        200    4    0.160000  0.840000   0.875000  0.700000   \n",
      "15         5        200   13    0.520000  0.480000   0.384615  0.500000   \n",
      "16         5        200   12    0.480000  0.520000   0.375000  0.300000   \n",
      "17         5        100    5    0.200000  0.800000   0.777778  0.700000   \n",
      "18         5         50    6    0.240000  0.760000   0.833333  0.500000   \n",
      "19         5         50    4    0.160000  0.840000   0.875000  0.700000   \n",
      "20         5        100   10    0.400000  0.600000   0.500000  0.500000   \n",
      "21        15        200   26    0.115556  0.884444   0.620690  0.545455   \n",
      "22        50        200  109    0.043600  0.956400   0.511905  0.387387   \n",
      "23         5       1200   10    0.400000  0.600000   0.500000  0.500000   \n",
      "24        10       1200   19    0.190000  0.810000   0.550000  0.523810   \n",
      "25         5       5000    9    0.360000  0.640000   0.555556  0.500000   \n",
      "26         5        200    3    0.120000  0.880000   0.818182  0.900000   \n",
      "27         5       5000   10    0.400000  0.600000   0.500000  0.700000   \n",
      "\n",
      "     f1score  total excution time           dataset  \n",
      "0   0.705882             0.030231   timeseries1.csv  \n",
      "1   0.900000             0.038324  timeseries10.csv  \n",
      "2   0.511628             0.092709  timeseries11.csv  \n",
      "3   0.666667             0.078345  timeseries12.csv  \n",
      "4   0.636364             0.026090  timeseries13.csv  \n",
      "5   0.600000             0.027241  timeseries14.csv  \n",
      "6   0.500000             0.029522  timeseries15.csv  \n",
      "7   0.608696             0.027209  timeseries16.csv  \n",
      "8   0.536585             0.067797  timeseries17.csv  \n",
      "9   0.526316             0.027507  timeseries18.csv  \n",
      "10  0.761905             0.158123  timeseries19.csv  \n",
      "11  0.564103             0.077026   timeseries2.csv  \n",
      "12  0.761905             0.160745  timeseries20.csv  \n",
      "13  0.705882             0.025814  timeseries21.csv  \n",
      "14  0.777778             0.029094  timeseries22.csv  \n",
      "15  0.434783             0.026952  timeseries23.csv  \n",
      "16  0.333333             0.025851  timeseries24.csv  \n",
      "17  0.736842             0.025662  timeseries25.csv  \n",
      "18  0.625000             0.021661  timeseries26.csv  \n",
      "19  0.777778             0.023216  timeseries27.csv  \n",
      "20  0.500000             0.023151  timeseries28.csv  \n",
      "21  0.580645             0.143568   timeseries3.csv  \n",
      "22  0.441026             3.194569   timeseries4.csv  \n",
      "23  0.500000             0.077969   timeseries5.csv  \n",
      "24  0.536585             0.188862   timeseries6.csv  \n",
      "25  0.526316             0.475749   timeseries7.csv  \n",
      "26  0.857143             0.027178   timeseries8.csv  \n",
      "27  0.583333             0.492556   timeseries9.csv  \n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lingam\n",
    "import time\n",
    "\n",
    "# 定义处理单个数据集的函数\n",
    "def process_dataset(timeseries_file, groundtruth_file, threshold=0.1):\n",
    "    \n",
    "    X = pd.read_csv(timeseries_file)\n",
    "    n_samples = X.shape[0]\n",
    "    model = lingam.VARLiNGAM(lags=2)\n",
    "\n",
    "    start_time = time.time()\n",
    "    model.fit(X)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "\n",
    "    adjacency_matrices = model.adjacency_matrices_\n",
    "\n",
    "    result = np.zeros_like(adjacency_matrices[0])\n",
    "    for idx, adj_matrix in enumerate(adjacency_matrices):\n",
    "        result += np.abs(adj_matrix)\n",
    "\n",
    "    result = (result > threshold).astype(int)\n",
    "\n",
    "    data = pd.read_csv(groundtruth_file, header=None)\n",
    "    data.columns = ['cause', 'effect', 'edge']\n",
    "\n",
    "    num_causes = data['cause'].max() + 1\n",
    "    num_effects = data['effect'].max() + 1\n",
    "    dim = max(num_causes, num_effects)\n",
    "\n",
    "    causality_matrix = np.zeros((dim, dim), dtype=int)\n",
    "    for _, row in data.iterrows():\n",
    "        causality_matrix[row['effect'], row['cause']] = row['edge']\n",
    "\n",
    "    precision, recall, f1_score, accuracy = precision_recall_f1(causality_matrix, result)\n",
    "    shd = structural_hamming_distance(causality_matrix, result)\n",
    "    error_rate = shd / (dim ** 2)\n",
    "\n",
    "    return {\n",
    "        \"features\": dim,\n",
    "        \"n_samples\": n_samples,\n",
    "        \"SHD\": shd,\n",
    "        \"error rate\": error_rate,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1score\": f1_score,\n",
    "        \"total excution time\": execution_time\n",
    "    }\n",
    "\n",
    "# 定义计算指标的辅助函数\n",
    "def precision_recall_f1(matrix1, matrix2):\n",
    "    assert matrix1.shape == matrix2.shape, \"Matrices must have the same shape\"\n",
    "\n",
    "    bool_matrix1 = matrix1 != 0\n",
    "    bool_matrix2 = matrix2 != 0\n",
    "\n",
    "    TP = np.sum(np.logical_and(bool_matrix1, bool_matrix2))\n",
    "    FP = np.sum(np.logical_and(np.logical_not(bool_matrix1), bool_matrix2))\n",
    "    FN = np.sum(np.logical_and(bool_matrix1, np.logical_not(bool_matrix2)))\n",
    "    TN = np.sum(np.logical_and(np.logical_not(bool_matrix1), np.logical_not(bool_matrix2)))\n",
    "\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) > 0 else 0\n",
    "\n",
    "    return precision, recall, f1_score, accuracy\n",
    "\n",
    "def structural_hamming_distance(matrix1, matrix2):\n",
    "    assert matrix1.shape == matrix2.shape, \"Matrices must have the same shape\"\n",
    "\n",
    "    bool_matrix1 = matrix1 != 0\n",
    "    bool_matrix2 = matrix2 != 0\n",
    "\n",
    "    distance = np.sum(bool_matrix1 != bool_matrix2)\n",
    "\n",
    "    return distance\n",
    "\n",
    "# 定义处理所有数据集的函数\n",
    "def process_all_datasets(path_input, path_groundtruth, output_file='results.csv', threshold=0.1):\n",
    "    files_input_name = [f for f in listdir(path_input) if isfile(join(path_input, f)) and not f.startswith('.')]\n",
    "    results = []\n",
    "\n",
    "    for file_input_name in files_input_name:\n",
    "        timeseries_file = join(path_input, file_input_name)\n",
    "        idx_ground_truth_file = file_input_name.split('timeseries')[1].split('.csv')[0]\n",
    "        file_ground_truth_name = \"sim\" + idx_ground_truth_file + \"_gt_processed.csv\"\n",
    "        groundtruth_file = join(path_groundtruth, file_ground_truth_name)\n",
    "\n",
    "        result = process_dataset(timeseries_file, groundtruth_file, threshold)\n",
    "        result[\"dataset\"] = file_input_name\n",
    "        results.append(result)\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "\n",
    "    # # Calculate variance and standard deviation for each metric\n",
    "    # metrics = [\"SHD\", \"error rate\", \"accuracy\", \"precision\", \"recall\", \"f1score\"]\n",
    "    # variance = {metric: np.var(results_df[metric]) for metric in metrics}\n",
    "    # std_deviation = {metric: np.std(results_df[metric]) for metric in metrics}\n",
    "\n",
    "    # # Add variance and standard deviation to the results DataFrame\n",
    "    # for metric in metrics:\n",
    "    #     results_df[f\"{metric}_variance\"] = variance[metric]\n",
    "    #     results_df[f\"{metric}_std_deviation\"] = std_deviation[metric]\n",
    "\n",
    "    # # Save updated results with variance and standard deviation\n",
    "    # results_df.to_csv(output_file, index=False)\n",
    "\n",
    "    return results_df\n",
    "\n",
    "# 路径设置\n",
    "path_input = './data/data_benchmark_1/fMRI_processed_by_Nauta/returns/'\n",
    "path_groundtruth = './data/data_benchmark_1/fMRI_processed_by_Nauta/ground_truth/'\n",
    "output_file = 'results.csv'\n",
    "\n",
    "# 处理所有数据集并保存结果\n",
    "results_df = process_all_datasets(path_input, path_groundtruth, output_file, threshold=0.2)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nS9WQHtlPe57",
    "outputId": "646ed7f8-2beb-4c39-d520-d1c19ffd6eb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6197515738247849\n",
      "0.13946696550768203\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(np.mean(results_df[\"f1score\"]))\n",
    "print(np.std(results_df[\"f1score\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
